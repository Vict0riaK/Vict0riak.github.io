---
layout: post
title: "Peer response 1 Collaborative Discussion 2: Legal and Ethical Views on ANN Applications"
subtitle: "Peer response 1"
categories: [machine-learning]

tags: [LLM, ANN]
---

## Initial Post by Kieron 

The realm of writing, long considered a bastion of human intellect and creativity, is now undergoing a profound transformation with the rise of "robo-writers." As Hutson (2021) compellingly details, these AI-powered language generators are no longer confined to sci-fi but are actively reshaping how we produce and consume text. This discussion invites us to critically examine the complex tapestry of risks and benefits woven by AI writers across diverse applications, from the mundane efficiency of administrative tasks to the nuanced artistry of creative expression. The inherent technical risks and uncertainties embedded within these systems warrant our close attention.

On the administrative frontier, AI writers herald an era of unprecedented efficiency and scalability. Imagine the swift generation of routine reports, the rapid drafting of standardised communications, or the instantaneous summarisation of vast datasets. AI can liberate human capital from repetitive linguistic chores, allowing professionals to dedicate their talents to strategic thinking and complex problem-solving (Russell and Norvig, 2021). This acceleration promises tangible gains in productivity, cost reduction, and a new level of consistency in high-volume textual output, a boon for any large organisation striving for streamlined operations.

However, this administrative revolution is not without its shadows: accuracy and algorithmic bias loom as formidable challenges. An AI-generated error in a financial report or legal document could have catastrophic consequences, while biases unwittingly baked into training data can perpetuate and even amplify societal inequalities in areas ranging from internal HR communications to public policy statements (Crawford, 2021). Furthermore, the subtle threat of deskilling human employees arises, fostering an over-reliance on AI and potentially eroding critical thinking and nuanced communication abilities.

Shifting to the more ethereal domain of creative writing, the benefits morph into intriguing possibilities. AI writers can serve as powerful catalysts for inspiration, shattering writer's block with novel prompts, suggesting unexpected plot twists, or conjuring evocative descriptions (Hutson, 2021). They offer a sandbox for experimenting with diverse stylistic voices and could democratise creative expression, making literary pursuits more accessible. The dream of personalised narrative experiences, where AI crafts stories tailored to individual reader preferences, might also draw closer.

Yet, it is here that the deeper, more existential risks surface. What defines originality and authenticity when an algorithm can mimic any human style? The very notion of authorship becomes blurred, raising thorny questions of intellectual property and potential plagiarism if AI inadvertently reproduces copyrighted content or if human users are not transparent about AI's role. The spectre of deepfakes extending to an author's unique voice poses profound ethical dilemmas. Beyond these philosophical considerations, the very real threat of job displacement for human writers, editors, and journalists casts a long shadow over the creative industries. The inherent technical risk in these advanced AI models often lies in their "black box" nature; predicting their precise output or preventing the generation of harmful, nonsensical, or subtly manipulative content remains an ongoing, complex challenge.

In sum, AI writers stand at a pivotal intersection of progress and peril. Their deployment demands not just technological prowess but profound ethical foresight. To responsibly harness their formidable power, we must champion robust governance, continuous oversight, and unwavering transparency.

As AI's presence in writing solidifies, two pressing questions emerge:

As AI writing tools become increasingly sophisticated, who bears the primary responsibility when AI-generated content is biased or factually incorrect – the AI developer, the user, or the platform? How might we establish clearer lines of accountability?

Considering the evolving capabilities of AI writers, how do you foresee the long-term impact on human creativity and employment in traditionally text-heavy professions? What strategies can individuals and industries adopt to adapt to this shift?

**References**

Crawford, K. (2021) Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. New Haven: Yale University Press.

Hutson, M. (2021) ‘Robo-writers: The rise and risks of language-generating AI’, Nature, 598(7882), pp. 556-559. doi: 10.1038/d41586-021-02875-z.

Russell, S. J. and Norvig, P. (2021) Artificial Intelligence: A Modern Approach. 4th edn. Harlow: Pearson Education.

## Peer response 1  

Hi Kieron,

Thanks for raising a very important accountability question. I tend to agree with Jones (Jones, 2025) that the companies that create and train LLMs should be responsible for the content they create and that there should be a serious regulation around AI. Right now these companies are not (Jones, 2025). Think of a case with Grok posting hate speech on X which when prompted in a certain way. It spits out racist content and calls itself a megahitler. The only thing we know is that Grok was tuned to “not shy away from making claims which are politically incorrect.”(Jones, 2025) But we do not really know whether it would make a difference and if it was tuned "to be polite”? Another huge problem with LLMs is that they are not trustworthy and hallucinate (Jones, 2025) I think we should try to make them as trustworthy as possible and tackle these shortcomings.
It is a good question also about writers. Think about it – LLMs also plagiarize all the time. Do they cite the sources? - No. And since it is a black box really, we have little idea where the content comes from. Protection of the artists is a huge problem indeed but generally of Intellectual Property as well (Jones, 2025). The law is not really catching up with the changing reality and I believe it will take time until we have legislation in place to protect IP in the world with AI.

**References**

Jones, D. (2025). Why Grok fell in love with Hitler. Politico. https://www.politico.com/news/magazine/2025/07/10/musk-grok-hitler-ai-00447055 [accessed 20.07.2025]
